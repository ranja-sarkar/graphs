
Deep learning architectures that are GNNs at the core:

--> CNN
GNNs on graphs with translational symmetry in all dimensions are CNNs.

--> RNN
GNNs on one-dimemsional directed line graph dictating how the tokens of a sentence flow in are RNNs.

--> Transformer
The self-attention mechanism in decoders can be viewed as NN on a fully connected graph on all tokens of the context window. 
For causal self-attention, they can be thought of as special directed graph where one token is connected to all previous tokens 
in the context window.

Outside of the core NN architecture, graphs can push the performance of model by giving it access to additional knowledge about the world. 
Retrieval augmented generation (RAG) currently is a go-to approach to reduce hallucination of LLMs by giving them access to new information.
Typically this is done by text or tabular data identified via vector similarity. 

On the other hand, data stored in graph databases is much better suited as input here due to their ability to provide the complex relationships 
between objects. This is why systems leveraging graphRAG are on the rise.
